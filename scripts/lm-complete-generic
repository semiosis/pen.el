#!/bin/bash

# Example of usage:
# export OPENAI_API_KEY
# export PEN_ environment variables
# openai-complete.sh

sn="$(basename "$0")"

case "$sn" in
    ai21-complete.sh) {
        completer_sn=pen-ai21
        : "${AI21_API_KEY:="$PEN_LM_KEY"}"
        export AI21_API_KEY

        : "${PEN_MODEL:="j1-jumbo"}"
        : "${PEN_MODEL:="j1-large"}"
    }
    ;;

    offline-complete.sh) {
        completer_sn=pen-offline
        : "${OFFLINE_API_KEY:="$PEN_LM_KEY"}"
        export OFFLINE_API_KEY

        : "${PEN_MODEL:="DummyModel"}"
    }
    ;;

    human-complete.sh) {
        completer_sn=pen-human
        : "${HUMAN_API_KEY:="$PEN_LM_KEY"}"
        export HUMAN_API_KEY

        : "${PEN_MODEL:="DummyModel"}"
    }
    ;;

    alephalpha-complete.sh) {
        completer_sn=pen-alephalpha
        : "${ALEPHALPHA_API_KEY:="$PEN_LM_KEY"}"
        export ALEPHALPHA_API_KEY

        : "${PEN_MODEL:="EUTranMultimodal"}"
    }
    ;;

    aix-complete.sh) {
        completer_sn=pen-aix
        : "${AIX_API_KEY:="$PEN_LM_KEY"}"
        export AIX_API_KEY

        : "${PEN_MODEL:="GPT-J-6B"}"
    }
    ;;

    hf-complete.sh) {
        completer_sn=pen-hf
        : "${HF_API_KEY:="$PEN_LM_KEY"}"
        export HF_API_KEY

        : "${PEN_MODEL:="gpt2"}"
    }
    ;;

    nlpcloud-complete.sh) {
        completer_sn=pen-nlpcloud
        : "${NLPCLOUD_API_KEY:="$PEN_LM_KEY"}"
        export NLPCLOUD_API_KEY

        : "${PEN_MODEL:="gpt-j"}"
        : "${PEN_MODEL:="gpu/gpt-j"}"
    }
    ;;

    cohere-complete.sh|cohere-*) {
        completer_sn=pen-cohere
        : "${COHERE_API_KEY:="$PEN_LM_KEY"}"
        export COHERE_API_KEY

        : "${PEN_MODEL:="large"}"
    }
    ;;

    openai-complete.sh|openai-*|*) {
        completer_sn=pen-openai
        : "${OPENAI_API_KEY:="$PEN_LM_KEY"}"
        export OPENAI_API_KEY

        # Default for OpenAI is davinci
        : "${PEN_MODEL:="davinci"}"
        : "${PEN_MODEL:="curie"}"
    }
    ;;
esac

p () {
    {
        i=1
        while [ "$i" -lt "$#" ]; do
            eval ARG=\${$i}
            printf -- "%s " "$ARG"
            i=$((i + 1))
        done
        eval ARG=\${$i}
        printf -- "%s" "$ARG"
    } | sed 's/\\n/\n/g' | pen-restore-chars
}

openai_results_split() {
    completions_fp="$1"
    test -f "$completions_fp" || exit 1

    completions_fp="$(realpath "$completions_fp")"

    td="$(mktemp -t "results_$(date-ts-hr)_XXXXX" -d -p ~/.pen/results)"

    cd "$td"

    if test "$PEN_MODE" = search; then
        if cat "$completions_fp" | grep -q -P '^===== Completion [0-9]+ =====$'; then
            csplit -f splitfile_ -z "$completions_fp" "/^===== Completion [0-9]\\+ =====$/" '{*}' &>/dev/null
            for fp in *; do
                sed -i 1d "$fp"
            done
        else
            cat "$completions_fp" > splitfile_0.txt
        fi
    else
        if test "$PEN_N_COMPLETIONS" = 1; then
            tail -c "+$(( PEN_COLLECT_FROM_POS + 1 ))" "$completions_fp" > response.txt
        elif cat "$completions_fp" | grep -q -P '^===== Completion [0-9]+ =====$'; then
            csplit -f splitfile_ -z "$completions_fp" "/^===== Completion [0-9]\\+ =====$/" '{*}' &>/dev/null
            for fp in *; do
                sed -i 1d "$fp"
                tail -c "+$(( PEN_COLLECT_FROM_POS + 1 ))" "$fp" | pen-sponge "$fp"
            done
        else
            cat "$completions_fp" > splitfile_0.txt
        fi
    fi

    echo "$td"
}

nlpcloud_results_split() {
    completions_fp="$1"
    test -f "$completions_fp" || exit 1

    completions_fp="$(realpath "$completions_fp")"

    td="$(mktemp -d)"

    cd "$td"

    if cat "$completions_fp" | grep -q -P '^--------------$'; then
        csplit -f splitfile_ -z "$completions_fp" "/^--------------$/" '{*}' &>/dev/null
        for fp in *; do
            sed -i 1d "$fp"
            # printf -- "%s" "$PEN_PROMPT$PEN_TRAILING_WHITESPACE"
            re="$(printf -- "%s" "$PEN_TRAILING_WHITESPACE" | sed -z 's/././g')"
            sed -i -z "s/^$re//" "$fp"
        done
    else
        cat "$completions_fp" > splitfile_0.txt
    fi

    echo "$td"
}

if test "$PEN_DEBUG" = "y"; then
    echo "PEN_USER_AGENT:\"$PEN_USER_AGENT\""
    echo "PEN_PROMPT:\"$PEN_PROMPT\""
    echo "PEN_PAYLOADS:\"$PEN_PAYLOADS\""
    echo "PEN_WHITESPACE_SUPPORT:\"$PEN_WHITESPACE_SUPPORT\""
    echo "PEN_CACHE:\"$PEN_CACHE\""
    echo "PEN_MODEL:\"$PEN_MODEL\""
    echo "PEN_FLAGS:\"$PEN_FLAGS\""
    echo "PEN_APPROXIMATE_PROMPT_LENGTH:\"$PEN_APPROXIMATE_PROMPT_LENGTH\""
    echo "PEN_MIN_TOKENS:\"$PEN_MIN_TOKENS\""
    echo "PEN_MAX_TOKENS:\"$PEN_MAX_TOKENS\""
    echo "PEN_MIN_GENERATED_TOKENS:\"$PEN_MIN_GENERATED_TOKENS\""
    echo "PEN_MAX_GENERATED_TOKENS:\"$PEN_MAX_GENERATED_TOKENS\""
    echo "PEN_ENGINE_MIN_TOKENS:\"$PEN_ENGINE_MIN_TOKENS\""
    echo "PEN_ENGINE_MAX_TOKENS:\"$PEN_ENGINE_MAX_TOKENS\""
    echo "PEN_ENGINE_MAX_GENERATED_TOKENS:\"$PEN_ENGINE_MAX_GENERATED_TOKENS\""
    echo "PEN_STOP_SEQUENCE:\"$PEN_STOP_SEQUENCE\""
    echo "PEN_STOP_SEQUENCES:\"$PEN_STOP_SEQUENCES\""
    echo "PEN_REPETITION_PENALTY:\"$PEN_REPETITION_PENALTY\""
    echo "PEN_TOP_P:\"$PEN_TOP_P\""
    echo "PEN_TOP_K:\"$PEN_TOP_K\""
    echo "PEN_N_COMPLETIONS:\"$PEN_N_COMPLETIONS\""
    echo "PEN_COLLECT_FROM_POS:\"$PEN_COLLECT_FROM_POS\""
    echo "PEN_END_POS:\"$PEN_END_POS\""
    # echo "PEN_QUERY_POS:\"$PEN_QUERY_POS\""
    echo "PEN_ENGINE_MAX_N_COMPLETIONS:\"$PEN_ENGINE_MAX_N_COMPLETIONS\""
    exit 1
fi

# tf_prompt="$(mktemp -t "lm_results_XXXXXX.txt" 2>/dev/null)"
# trap "rm \"$tf_prompt\" 2>/dev/null" 0

# Default for OpenAI is davinci
: "${PEN_MODEL:="davinci"}"
: "${PEN_MODEL:="curie"}"

# No prompt should be allowed
# test -n "$PEN_PROMPT" || {
#     echo No prompt given
#     exit 1
# }

: "${PEN_MIN_TOKENS:="20"}"
: "${PEN_MAX_TOKENS:="60"}"
: "${PEN_ENGINE_MIN_TOKENS:="20"}"
: "${PEN_ENGINE_MAX_TOKENS:="60"}"
: "${PEN_TEMPERATURE:="0.8"}"
: "${PEN_TOP_P:="1"}"
: "${PEN_TOP_K:="10"}"
: "${PEN_N_COMPLETIONS:="1"}"

export PEN_USER_AGENT
export PEN_WHITESPACE_SUPPORT
export PEN_LM_KEY
export PEN_API_ENDPOINT
export PEN_MODEL
export PEN_PROMPT
export PEN_PAYLOADS
export PEN_FLAGS
export PEN_MIN_TOKENS
export PEN_MAX_TOKENS
export PEN_APPROXIMATE_PROMPT_LENGTH
export PEN_MIN_GENERATED_TOKENS
export PEN_MAX_GENERATED_TOKENS
export PEN_ENGINE_MIN_TOKENS
export PEN_ENGINE_MAX_TOKENS
export PEN_TEMPERATURE
export PEN_TOP_P
export PEN_TOP_K # best of
export PEN_STOP_SEQUENCE
export PEN_STOP_SEQUENCES
export PEN_REPETITION_PENALTY
export PEN_LENGTH_PENALTY
export PEN_CACHE
export PEN_MODE
export PEN_ENGINE_MAX_GENERATED_TOKENS
export PEN_ENGINE_MAX_N_COMPLETIONS
# Some backends strip whitespace. Recreate it.
export PEN_TRAILING_WHITESPACE
export PEN_COLLECT_FROM_POS
export PEN_END_POS
# export PEN_QUERY_POS

tf_response="$(mktemp -t "lm_results_XXXXXX.txt" 2>/dev/null)"
trap "rm \"$tf_response\" 2>/dev/null" 0

# This evaluates the newlines without trimming the prompt at the ends
IFS= read -rd '' PEN_PROMPT < <(p "$PEN_PROMPT");typeset -p PEN_PROMPT &>/dev/null

# This is designed to not trim whitespace from the ends of the stop sequence
IFS= read -rd '' PEN_STOP_SEQUENCE < <(p "$PEN_STOP_SEQUENCE" | pen-str unonelineify-safe);typeset -p PEN_STOP_SEQUENCE &>/dev/null

# Will it complain if PEN_STOP_SEQUENCE is empty?

case "$sn" in
    cohere-complete.sh|cohere-*) {
        if test "$LM_COMPLETE_DEBUG" = "y"; then
            "$completer_sn"
            exit $?
        else
            "$completer_sn"  > "$tf_response"
        fi
    }
    ;;

    openai-complete.sh|openai-*) {
        # TODO: Make a new completer

        # - BEST_OF
        # - TOP_P
        # - FREQUENCY_PENALTY
        # - PRESENCE_PENALTY
        # - Utilise all stop sequences

        # "$completer_sn" api \
        #     completions.create \
        #     -e "$PEN_MODEL" \
        #     -t "$PEN_TEMPERATURE" \
        #     -P "$PEN_TOP_P" \
        #     -M "$PEN_MAX_GENERATED_TOKENS" \
        #     -n "$PEN_N_COMPLETIONS" \
        #     --stop "$PEN_STOP_SEQUENCE" \
        #     -p "$PEN_PROMPT" > "$tf_response"

        # pen-openai-complete > "$tf_response"

        if test "$LM_COMPLETE_DEBUG" = "y"; then
            "$completer_sn"
            exit $?
        else
            "$completer_sn"  > "$tf_response"
        fi
    }
    ;;

    # ai21-complete.sh
    # alephalpha-complete.sh
    # offline-complete.sh
    # aix-complete.sh
    # hf-complete.sh
    # nlpcloud-complete.sh
    *) {
        if test "$LM_COMPLETE_DEBUG" = "y"; then
            "$completer_sn"
            exit $?
        else
            "$completer_sn"  > "$tf_response"
        fi
    }
    ;;
esac

case "$sn" in
    nlpcloud-complete.sh) {
        export PEN_COLLECT_FROM_POS
        results_dir="$(nlpcloud_results_split "$tf_response")"
    }
    ;;

    human-complete.sh) {
        : "${PEN_COLLECT_FROM_POS:="$(cat "$tf_response" | wc -c)"}"

        export PEN_COLLECT_FROM_POS
        results_dir="$(openai_results_split "$tf_response")"
        echo "human" > stop-reason.txt
    }
    *) {
        : "${PEN_COLLECT_FROM_POS:="$(cat "$tf_response" | wc -c)"}"

        export PEN_COLLECT_FROM_POS
        results_dir="$(openai_results_split "$tf_response")"
    }
    ;;
esac

# The API returns the entire prompt + completion
# Which seems a little bit wasteful.
# That may change.
# sp +/"^Echo" "$NOTES/ws/openai/api/glossary.txt"
# It's the openai cli which doesn't yet have an echo parameter

# jsonl output would be nice but jq is an extra dependency I don't want.
# Alternatively, can could make it linewise.

echo "$results_dir"
