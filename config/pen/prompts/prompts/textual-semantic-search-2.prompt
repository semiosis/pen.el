task: "textual semantic search"
tags: search nlp-task
doc: "Given a document, measure how related it is to a query"
# TODO 
external-related: "https://github.com/openai/openai-python/blob/main/examples/semanticsearch/semanticsearch.py"
engine: OpenAI Codex
force-engine: OpenAI Codex
prompt-version: 1
prompt: |+
    <|endoftext|><document>
    
    ---
    
    The above passage is related to: <:pp><query>
# In search mode, pp is used to collect token logprompts following.
# in semantic search the trailing tokens have their logprobs added up and averaged.
# TODO: Send multiple prompts in same request.
# But how are they interactively wrapped up as input?
packable: on
package-delimiter: "\n"
temperature: 0.3
top-p: 1.0
stop-sequences:
- "\n"
cache: on
mode: search
search-threshold: 50
### Search means to return logprob and force max-generated-tokens to 0
# force-logprobs: 0
# force-max-generated-tokens: 0
# # I also need to bypass any checks
# force-api: on
# Also, change the return function from prompts to logprob
vars:
- "query"
examples:
- "is a todo item"
info: on
filter: off
completion: off
insertion: off