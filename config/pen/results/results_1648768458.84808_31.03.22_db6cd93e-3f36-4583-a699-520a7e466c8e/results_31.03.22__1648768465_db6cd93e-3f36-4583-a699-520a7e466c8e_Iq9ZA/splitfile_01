
Generative Pre-trained Transformer is a language model that uses deep learning to produce human-like text. \n\nIt is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI,\na San Francisco-based artificial intelligence research laboratory. GPT-3's full version has a capacity of 175 billion\nmachine learning parameters. GPT-3 is currently in beta testing and was introduced in May 2020.[1] \n
