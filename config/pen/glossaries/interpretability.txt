TCAV
Testing with Concept Activation Vectors
    [model interpretability technique]

    Interpretability Beyond Feature Attribution.

    https://towardsdatascience.com/tcav-interpretability-beyond-feature-attribution-79b4d3610b4d

    Used to understand what signals the NN
    models use for their prediction.

    Let’s say we have a model that is trained
    to detect zebras from images.
    
    We would want to know which variables have
    played a role in deciding whether the
    image was a zebra or not.
    
    TCAV can help us understand if the concept
    of stripes was essential to the model’s
    prediction, which is actually yes in this
    case.

    TCAV uses directional derivatives to
    quantify the degree to which a user-
    defined idea is vital to a classification
    result–for example, how sensitive a
    prediction of “zebra” is to the presence
    of stripes.

Concept Activation Vector
CAV
    CAVs provide an interpretation of a neural
    net’s internal state in terms of human-
    friendly concepts.