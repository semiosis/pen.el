neural hash
learn-to-hash
    https://www.sajari.com/blog/vectors-versus-hashes/

    Replace existing LSH techniques with
    hashes created by NNs.
    
    The resulting hashes can be compared using
    the very fast Hamming distance calculation
    to estimate their similarity.

locality-sensitive hashing
locality sensitive hashing
LSH
    An algorithmic technique that hashes
    similar input items into the same
    "buckets" with high probability.

    The number of buckets are much smaller
    than the universe of possible input
    items.
    
    Since similar items end up in the same
    buckets, this technique can be used for
    data clustering and nearest neighbor
    search.

    It differs from conventional hashing
    techniques in that hash collisions are
    maximized, not minimized.
    
    Alternatively, the technique can be seen
    as a way to reduce the dimensionality of
    high-dimensional data; high-dimensional
    input items can be reduced to low-
    dimensional versions while preserving
    relative distances between items.
    
    Hashing-based approximate nearest neighbor
    search algorithms generally use one of two
    main categories of hashing methods: either
    data-independent methods, such as
    locality-sensitive hashing (LSH); or data-
    dependent methods, such as Locality-
    preserving hashing (LPH).

    So it turns out binary comparisons like
    XOR on bit sets can be computed much, much
    faster than float based arithmetic. So
    what if you could represent the 0.65 and
    0.66 in a binary hash space that was
    locality sensitive? Could that make models
    much faster in terms of inference?

    Note: looking at a single number is a
    contrived example, but for vectors
    containing many floats the hash can
    actually also compress the relationship
    between all the dimensions which is where
    the magic really happens.

    Turns out there is a family of hash
    algorithms to do just this called locality
    sensitive hashing (LSH). The closer the
    original items, the closer the bits in
    their hashes are the same.

Fibonacci hashing
    Hash tables should not:
    - be prime number sized
    - use an integer modulo to map hashes into
      slots.

    Fibonacci hashing is just better, yet
    somehow nobody is using it.

    Lots of big hash tables (including all the
    big implementations of std::unordered_map)
    are much slower than they should be
    because they donâ€™t use Fibonacci Hashing.
