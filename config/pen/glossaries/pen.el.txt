monad
    In the thought of Gottfried Wilhelm
    Leibniz, one of the ultimate indivisible
    units of existence.

    Monads are independent of one another and
    innately have the power of action and
    direction toward some end (see nisus).

    Although no monad in reality acts on any
    other, they work in a divinely
    preestablished harmony so that an
    appearance of causal connection is
    maintained.

    The concept of the monad was intended, in
    part, to address the mind-body problem
    arising from Cartesian dualism.

    "Pen.el is my monad of metaversal
    consciousness"

rhizome
    [philosophical concept]

    A concept in post-structuralism describing
    a nonlinear network that "connects any
    point to any other point".
    
    It appears in the work of French theorists
    Deleuze and Guattari, who used the term in
    their book A Thousand Plateaus to refer to
    networks that establish "connections
    between semiotic chains, organizations of
    power, and circumstances relative to the
    arts, sciences and social struggles" with
    no apparent order or coherency.
    
    A rhizome is purely a network of
    multiplicities that are not arborescent
    (e.g. the idea of hypertext in literary
    theory) with properties similar to
    lattices.
    
    Deleuze referred to it as extending from
    his concept of an "image of thought" that
    he had previously discussed in Difference
    and Repetition.

    Developed by Gilles Deleuze and Félix
    Guattari in their Capitalism and
    Schizophrenia (1972–1980) project.

    It is what Deleuze calls an "image of
    thought", based on the botanical rhizome,
    that apprehends multiplicities.

rhizome
creeping rootstalk
    A horizontal underground plant stem
    capable of producing the shoot and root
    systems of a new plant.
    
    Rhizomes are used to store starches and
    proteins and enable plants to perennate
    (survive an annual unfavourable season)
    underground.

pensieve
    NLG: ""Pensieve" refers to an object used
    by a witch or wizard to examine memories
    and store them in the form of "phials" of
    light.

phial
    NLG: "Phial" refers to a small container
    with a narrow neck, which is used to hold
    a small amount of liquid such as a drug.

imemory
imemories
    Imaginary memories.

Wizard of Oz testing
    Testing in which the automated machine
    component is substituted by some form of
    human intervention but in such a way that
    the user participating in the test is
    unaware of the substitution.

prompter
    The agent doing the prompting.

    This could be a human or a computer
    program.

    The computer program could be a chatbot,
    for example.

promptee
    The agent being prompted.

    This could be a language model, a human or
    a prompt server.

prompt server
    A prompt server responds to prompting
    queries with a prompting reponse.

    A prompt server may wrap a language model,
    or consult a human, computer program or
    database.

    Pen.el is an example of a prompt server.

Yggdrasil
    [#Norse cosmology]

    An immense and central sacred tree.
    
    Around it exists all else, including the
    Nine Worlds.
    
    Yggdrasil is attested in the Poetic Edda
    compiled in the 13th century from earlier
    traditional sources, and in the Prose Edda
    written in the 13th century by Snorri
    Sturluson.

transvariadicator
    [#pen.el]

    The variadic arg transformer.

    This is an elisp function or a shell
    command to transform the variadic arg into
    final text which is inserted into the
    template.

prompt function
prompting function
f_prompt(·)
    Each .prompt file generates a function in
    emacs lisp.

    This generated function I will refer to as
    a prompt function.

nucleus sampling
top p sampling
    Compute the cumulative distribution and
    cut off as soon as the CDF exceeds P.
    
    Example:
        In a broad distribution, it may take
        the top 100 tokens to exceed top_p =
        .9.
        
        In a narrow distribution, we may
        already exceed top_p = .9.
    
    Still avoid sampling egregiously wrong
    tokens, but preserve variety when the
    highest scoring tokens have low
    confidence.

    A method of determining which tokens to
    use that looks at the combined probability
    of groups of tokens.

    See "Top P".

    v +/"Nucleus sampling" "$MYGIT/mullikine/gpt-2/src/sample.py"

    https://arxiv.org/pdf/1904.09751.pdf

Top P
    [#OpenAI API]
    [API setting]
    [GPT-3 prompt setting]
    [GPT hyperparameter]
    [#GPT]

    Floating point value.

    Max value: 1
    Recommended value: 1
    Min value: 0
    
    https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277

    Diversity via nucleus sampling.

    Cuts off tokens below a certain
    probability threshold.

    We generally recommend keeping it set to
    around 1.

    Similar to temperature, this is an API
    setting that controls the diversity of
    tokens considered using nucleus sampling.
    
    A lower setting results in less random
    completions a higher setting is less
    deterministic and repetitive.
    
    Lower settings are generally better for
    classification tasks and higher settings
    for more open-ended responses.

    Controls diversity via nucleus sampling:
    1.5 means half of all likelihood-weighted
    options are considered.

    See "nucleus sampling".

prompt description
prompt description file
prompt description YAML
.prompt
    [#pen.el] 

    A yaml containing the definition of an
    individual prompt.

    Parameters:
    - model
      [string]

    - prompt
      [string]

    - Best of
      [int]

      Recommended value: 1
      Min value: 1
      Max value: 20

    - Frequency Penalty
      [float]

      Default value: 0
      Min value: 0
      Max value: 1

    - top-p
      [float]

      Recommended value: 1
      Max value: 1
      Min value: 0

    - temperature
      [float]

      Recommended value: 0.9 
      Min value: 0
      Max value: 1

    - restart sequence
      [string]

    - start sequence
      [string]

    - stop sequences
      [list of strings]

    - response length
      [int]

      Min value: 64
      Max value: 2048

    - presence penalty
      [float]

      Default value: 0
      Min value: 0
      Max value: 1

chaining
    [#prompt engineering]

    http://gptprompts.wikidot.com/chaining:chaining

    API results into one another: The results
    of one API call can be processed and fed
    back into another API call with a
    different prompt to expand the potential
    capabilities of a workflow.
        
    While OpenAI API has a lot of capabilities
    on its own, it is limited to 2000
    characters per prompt.
    
    This puts a need to be able to determine
    optimal types of prompt selection and
    examples included in the prompt for
    complex queries.
    
    Rather than try to do everything at once,
    subtasks can be delegated to different API
    calls with subtask-optimized prompts.
    
    I don't know if there's a better term for
    this, so I'm tentatively calling this
    behavior chaining and can change this if
    it turns out there's a better name.
    
    Chaining ends up boiling down into a
    couple different tasks.
    
    First, you need to figure out what a
    prompt is actually asking for.
    
    To do this you can decompose the task into
    its component parts and figure out the
    sort of prompts you need to query to
    answer the parts http://gptprompts.wikidot.com/chaining:using-the-fringe.
    
    For each of those sub-parts, you need to
    be able to figure out how to optimize the
    prompt so that it optimizes the answer via
    context stuffing
    http://gptprompts.wikidot.com/context-stuffing.
    
    However, during all of this, you want to
    make sure that you're ensuring that the
    API is in compliance with safety practices
    for your task
    http://gptprompts.wikidot.com/safety.
    
context-stuffing
    [#prompt engineering]

    With only 2048 tokens, you need to make
    use of your real estate by providing
    instructions and making implicit
    information explicit.

benchmarking
    [#prompt engineering]

    A single prompt getting a good response
    once might be enough for your use case,
    but generally we want to know how well
    different solutions hold up on a variety
    of use cases.
    
    Also important to understand where prompt
    designs outperform others.

explicit bias
    [#ai safety]

    Where output is clearly toxic (cursing,
    slander).

implicit bias
    [#ai safety]

    Where the policy from the output changes
    based on context (e.g. opposition to
    programs that help certain groups,
    probabilities about roles).

openai-ft
    [cli command]

    Run a fine-tuning job using OpenAI
    finetuning API.

    py -36 i openai-finetune

openai-ft-classification
    [cli command]

    Run a classification fine-tuning job using
    OpenAI finetuning API

    py -36 i openai-finetune

openai-ft-events
    [cli command]

    List the events for a batch-mode
    fine-tuning run.

    py -36 i openai-finetune

openai-ft-report
    [cli command]

    List the events for a batch-mode
    fine-tuning run.

    py -36 i openai-finetune

PEN_RESULT
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The text that begins at
    PEN_COLLECT_FROM_POS (before
    PEN_INJECT_GEN_START) but has even more
    text.

PEN_COLLECT_FROM_POS
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    Just before the URL.

PEN_END_POS
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The length of the prompt including PEN_INJECT_GEN_START.

    The end of the prompt (including the
    PEN_INJECT_GEN_START, which is the last
    part of the prompt).

PEN_INJECT_GEN_START
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The text that begins just after
    PEN_COLLECT_FROM_POS. There is no recorded
    byte position. It was the previous
    generation.

PEN_PROMPT
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The entire prompt including the
    PEN_INJECT_GEN_START string.

garble
    Reproduce (a message, sound, or
    transmission) in a confused and distorted
    way.
    "the connection was awful and kept
    garbling his voice"

garbify
    [#pen]

    ascify then remove whitespace. This is to
    make a block of text ready for
    classification but to avoid
    breaking/confusing prompts with their
    structure.

    Good for screenshots.